<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
  <div class="tocheadb">
    <h1 class="tochead" id="heading_id_2"><a id="pgfId-1075261"></a><a id="pgfId-1107519"></a>14 <a id="id_4cwx06g7tma4"></a>Fitting functions to data</h1>
  </div>

  <p class="co-summary-head"><a id="pgfId-1108434"></a>This chapter covers</p>

  <ul class="calibre8">
    <li class="co-summary-bullet"><a class="calibre10" id="pgfId-1108435"></a>Measuring how closely a function models a data set</li>

    <li class="co-summary-bullet"><a class="calibre10" id="pgfId-1108436"></a>Exploring spaces of functions determined by constants</li>

    <li class="co-summary-bullet"><a class="calibre10" id="pgfId-1108437"></a>Using gradient descent to optimize the quality of “fit”</li>

    <li class="co-summary-bullet"><a class="calibre10" id="pgfId-1108438"></a>Modeling data sets with different kinds of functions</li>
  </ul>

  <p class="body"><a id="pgfId-1107525"></a>The calculus techniques you learned in part 2 require well-behaved functions to be applicable. For a derivative to exist, a function needs to be sufficiently smooth, and to calculate an exact derivative or integral, you need a function to have a simple formula. For most real-world data, we aren’t so lucky. Due to randomness or measurement error, we rarely come across perfectly smooth functions in the wild. In this chapter, we cover how to take messy data and model it with a simple mathematical<a id="marker-1107526"></a> function−a task called <i class="fm-italics">regression</i>.</p>

  <p class="body"><a id="pgfId-1107527"></a>I’ll walk you through an example on a real data set, consisting of 740 used cars listed for sale on the website CarGraph.com. These cars are all Toyota Priuses, and they all have mileage and sale price reported. Plotting this data on a scatter plot, figure 14.1 shows that we can see there’s a downward trend in price as mileage increases. This reflects that cars lose value as they are driven. Our goal is to come up with a simple function that describes how the price of a used Prius changes as its mileage increases.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F01_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116114"></a>Figure 14.1 A plot of price vs. mileage for used Toyota Priuses listed for sale on CarGraph.com</p>

  <p class="body"><a id="pgfId-1107533"></a>We can’t draw a smooth function that passes through all of these points, and even if we could, it would be nonsensical. Many of these<a id="marker-1107534"></a> points are <i class="fm-italics">outliers</i> and probably erroneous (for instance, the handful of nearly new cars that are selling for less than $5,000 in figure 14.1). And there are certainly other factors that affect the resale price of a used car. We shouldn’t expect mileage alone to put an exact value on the price.</p>

  <p class="body"><a id="pgfId-1107535"></a>What we can do is find a function that approximates the trend of this data. Our function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) takes mileage <i class="fm-in-times-italic">x</i> as an input and returns a typical price for a Prius with that given mileage. To do this, we need to make a <i class="fm-italics">hypothesis</i> about what<a id="marker-1107536"></a> kind of function this will be. We can start with the simplest possible example: a linear function.</p>

  <p class="body"><a id="pgfId-1107537"></a>We looked at linear functions in many forms in chapter 7, but in this chapter, we’ll write these in the format <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i>, where <i class="fm-in-times-italic">x</i> is the mileage of a car, <i class="fm-in-times-italic">p</i> is its price, and <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> are numbers that determine the shape of the function. With a choice of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, this function is an imaginary machine that takes the mileage of a Toyota Prius and predicts its price as shown in figure 14.2.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/w_gifs_715.gif"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107542"></a>Figure 14.2 A schematic of a linear function predicting price p from mileage x</p>

  <p class="body"><a id="pgfId-1107543"></a>Remember, <i class="fm-in-times-italic">a</i> is the slope of the line and <i class="fm-in-times-italic">b</i> is its value at zero. With values like <i class="fm-in-times-italic">a</i> = <span class="fm-in-cambria">−</span>0.05 and <i class="fm-in-times-italic">b</i> = 20,000, the graph of the function becomes a line starting at a price of $20,000 and decreasing by $0.05 every time the mileage increases by one mile (figure 14.3).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/w_gifs_716.gif"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107548"></a>Figure 14.3 Predicting the price of a Prius based on the mileage, using a function of the form <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> with <i class="fm-in-times-italic">a</i> = <span class="fm-in-cambria">−</span>0.05 and <i class="fm-in-times-italic">b</i> = 20,000</p>

  <p class="body"><a id="pgfId-1107549"></a>This choice of prediction function implies a new Prius is worth $20,000, and that it depreciates or loses value at a rate of $0.05 per mile. These values may or may not be correct; in fact, we have reason to believe they aren’t perfect because the graph of this line doesn’t come close to most of the data. The task of finding the values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> so that <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) follows the trend of the data as well as possible<a id="marker-1107550"></a> is called <i class="fm-italics">linear regression</i>. Once we find<a id="marker-1107551"></a> the best values, we can say <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) is the <i class="fm-italics">line of best fit</i>.</p>

  <p class="body"><a id="pgfId-1107552"></a>If <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) is going to come close to the real data, it seems reasonable that slope <i class="fm-in-times-italic">a</i> should be negative so that the predicted price decreases as mileage increases. We won’t have to assume that, however, because we can implement an algorithm that figures this out directly from the raw data. This is why regression is a simple example of a machine learning algorithm; based on data alone, it infers a trend and can then make predictions about new data points.</p>

  <p class="body"><a id="pgfId-1107553"></a>The only real constraint we impose is that our algorithm looks for linear functions. A <i class="fm-italics">linear function</i> assumes that<a id="marker-1107554"></a> the rate of depreciation is constant−that the loss in dollar value of the car in its first 1,000 miles is the same as the loss in value during 100,000 to 101,000 miles. Conventional wisdom says this is not the case, and in fact, that cars lose a good portion of their value the moment they are driven off the lot. Our goal will not be to find the perfect model, but to find a simple model that still performs well.</p>

  <p class="body"><a id="pgfId-1107555"></a>The first thing we need to do is be able to measure how well a given linear function, meaning a given choice of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, predicts the price of a Prius from its mileage. To do this, we write a function in<a id="marker-1107556"></a> Python, called a <i class="fm-italics">cost function</i>, which takes a function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) as input and returns a number telling us how far it is from the raw data. For any pair of numbers <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, we can then measure how well the function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> fits the data set using the cost function. There’s one linear function for every pair (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>), so we can think of our task as exploring the 2D space of such pairs and evaluating the linear function they imply.</p>

  <p class="body"><a id="pgfId-1107557"></a>Figure 14.4 shows that picking positive values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> yield a line, sloping upwards. If that were our price function, it would imply that a car gains value as it is driven, which is not likely.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F04_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107562"></a>Figure 14.4 A pair of numbers (<i class="fm-in-times-italic">a, b</i>) define a linear function that we can plot on a graph as a line. For positive values of a, the graph slopes upward.</p>

  <p class="body"><a id="pgfId-1107563"></a>Our cost function compares a line like this to the actual data and returns a big number, indicating that the line is far away from the data. The closer the line gets to the data, the lower the cost and the better the fit.</p>

  <p class="body"><a id="pgfId-1107564"></a>What we want are values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> that don’t just make the cost function small, but that make it the <i class="fm-italics">exact</i> smallest function possible. The second major function we’ll write is called <code class="fm-code-in-text">linear_regression,</code> and it automatically finds these best values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>. This, in turn, tell us the line of best fit. To implement this, we build a function telling us the cost for any values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> and minimize it using the technique of gradient descent from chapter 12. Let’s get started by implementing a cost function in Python to measure how well a function fits a data set.</p>

  <h2 class="fm-head" id="heading_id_3"><a id="pgfId-1107566"></a><a id="id_p3s61itj8zt3"></a>14.1 Measuring the quality of fit for a function</h2>

  <p class="body"><a id="pgfId-1107567"></a>We’ll write our cost function so that it can work on any data set, not just our collection of used cars. That allows us to test it out on simpler (made-up) data sets, so we can see how it works. With that in mind, the cost function is a Python function taking two inputs. One of these is the Python function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) that we want to test, and the second is the data set to test against, a collection of (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) pairs. For the used car example, our <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) might be a linear function giving a cost in dollars for any mileage, and the (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) pairs are the actual values of mileage and price from the data set.</p>

  <p class="body"><a id="pgfId-1107568"></a>The output of the cost function is a single number, measuring how far the values of <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) are from the correct <i class="fm-in-times-italic">y</i> values. If <i class="fm-in-times-italic">y</i> = <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>), for every <i class="fm-in-times-italic">x</i>, the function is a perfect fit for the data, so the cost function returns zero. More realistically, the function won’t agree exactly with all the data points, and it will return some positive number. We’ll actually write two cost functions to compare them and give you a sense of how cost functions work:</p>

  <ul class="calibre8">
    <li class="fm-list-bullet2">
      <p class="list"><a id="pgfId-1107569"></a><code class="fm-code-in-text">sum_error</code> −Adds the distance from <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) to <i class="fm-in-times-italic">y</i> for every (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) value in the data set</p>
    </li>

    <li class="fm-list-bullet2">
      <p class="list"><a id="pgfId-1107570"></a><code class="fm-code-in-text">sum_square_error</code> −Adds up the squares of these distances</p>
    </li>
  </ul>

  <p class="body"><a id="pgfId-1107571"></a>The second of these is the cost function most commonly used in practice, and you’ll see why shortly.</p>

  <h3 class="fm-head1" id="heading_id_4"><a id="pgfId-1107573"></a><a id="id_62n9182cnwk1"></a>14.1.1 Measuring distance from a function</h3>

  <p class="body"><a id="pgfId-1107574"></a>In the source code for this book, you’ll find a made-up data set called <code class="fm-code-in-text">test_data</code>. It’s a Python list of (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) values, where the <i class="fm-in-times-italic">x</i> values range from −1 to 1. I’ve intentionally chosen <i class="fm-in-times-italic">y</i> values so that the points lie close to the line <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i>. Figure 14.5 shows a scatter plot of the <code class="fm-code-in-text">test_data</code> data set next to that line.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F05_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116220"></a>Figure 14.5 A set of randomly generated data that intentionally stays close to the line <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2x</p>

  <p class="body"><a id="pgfId-1107580"></a>The fact that <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i> stays close to the data set means that for any <i class="fm-in-times-italic">x</i> value in the data set, 2<i class="fm-in-times-italic">x</i> is a pretty good guess for the corresponding <i class="fm-in-times-italic">y</i> value. For instance, the point</p>

  <p class="body"><a id="pgfId-1107581"></a>(<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) = (0.2, 0.427)</p>

  <p class="body"><a id="pgfId-1107582"></a>is an actual value from the data set. Given only the value <i class="fm-in-times-italic">x</i> = 0.2, our <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i> would have predicted <i class="fm-in-times-italic">y</i> = 0.4. The absolute value of the difference |<i class="fm-in-times-italic">f</i>(0.2) − 0.4| tells us the size of this error, which is about 0.027.</p>

  <p class="body"><a id="pgfId-1107583"></a>An error value, which is the difference between the actual <i class="fm-in-times-italic">y</i> value and the one predicted by the function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>), can be pictured as the vertical distance from the actual (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) point to the graph of <i class="fm-in-times-italic">f</i> . Figure 14.6 shows the error distances drawn as vertical lines.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F06_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116264"></a>Figure 14.6 The error values are the differences between the function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) and the actual y values.</p>

  <p class="body"><a id="pgfId-1107589"></a>Some of these errors are smaller than others, but how can we quantify the quality of the fit? Let’s compare this to a picture of a function, <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) = 1 − <i class="fm-in-times-italic">x</i>, which is obviously a bad fit (figure 14.7).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F07_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116310"></a>Figure 14.7 Picturing a function with larger error values</p>

  <p class="body"><a id="pgfId-1107595"></a>Our function, <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) = 1 − <i class="fm-in-times-italic">x</i>, happens to come close to one of the points, but the total of the errors is much larger. For that reason, we can write our first cost function by adding all of the errors. A larger sum of errors means a worse fit, while a lower value means a better fit. To implement this function, we simply iterate over the (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) pairs, take the absolute value of the difference between <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) and <i class="fm-in-times-italic">y</i>, and sum the results:</p>
  <pre class="programlisting">def sum_error(f,data):
    errors = [abs(<i class="fm-in-times-italic1">f</i>(<i class="fm-in-times-italic1">x</i>) − y) for (x,y) in data]
    return sum(errors)</pre>

  <p class="body"><a id="pgfId-1107597"></a>To test this function, we can translate our <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) and <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) into code:</p>
  <pre class="programlisting">def f(x): 
    return 2*x

def <i class="fm-in-times-italic1">g</i>(<i class="fm-in-times-italic1">x</i>): 
    return 1-x</pre>

  <p class="body"><a id="pgfId-1107599"></a>As expected, the summed error for <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i> is lower than for <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) = 1 − <i class="fm-in-times-italic">x</i> :</p>
  <pre class="programlisting">&gt;&gt;&gt; sum_error(f,test_data)
5.021727176394801
&gt;&gt;&gt; sum_error(g,test_data)
38.47711311130152</pre>

  <p class="body"><a id="pgfId-1107601"></a>The exact values of these outputs don’t matter; what matters is the comparison between them. Because the sum of the error for <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) is lower than for <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>), we can conclude that <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) is a better fit for the given data.</p>

  <h3 class="fm-head1" id="heading_id_5"><a id="pgfId-1107603"></a>14.1.2 <a id="id_jvo7jcw1somj"></a>Summing the squares of the errors</h3>

  <p class="body"><a id="pgfId-1107604"></a>The <code class="fm-code-in-text">sum_error</code> function might be the most obvious way to measure the distance from the line to the data, but in practice, we’ll use a cost function that sums up the <i class="fm-italics">squares</i> of all the errors. There are a few good reasons for this. The simplest is because a squared distance function is smooth, so we can use derivatives to minimize it, while an absolute value function is not smooth, so we can’t take its derivative everywhere. Keep in mind the pictures of the graphs of functions |<i class="fm-in-times-italic">x</i>| and <i class="fm-in-times-italic">x</i><sup class="fm-superscript">2</sup> (figure 14.8), both of which return bigger values when <i class="fm-in-times-italic">x</i> is further from 0, but only the latter is smooth at <i class="fm-in-times-italic">x</i> = 0 and has a derivative there.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F08_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107609"></a>Figure 14.8 The graph of <i class="fm-in-times-italic">y</i> = |<i class="fm-in-times-italic">x</i>| is not smooth at <i class="fm-in-times-italic">x</i> = 0, but the graph of <i class="fm-in-times-italic">y</i> = <i class="fm-in-times-italic">x</i><sup class="fm-superscript">2</sup> is.</p>

  <p class="body"><a id="pgfId-1107610"></a>Given a test function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>), we can look at every (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) pair and add the value of (<i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) − <i class="fm-in-times-italic">y</i>)<sup class="fm-superscript">2</sup> to the cost. The <code class="fm-code-in-text">sum_squared_error</code> function does this, and its implementation doesn’t look much different than <code class="fm-code-in-text">sum_error</code>. We only need to square the error instead of taking its absolute value:</p>
  <pre class="programlisting">def sum_squared_error(f,data):
    squared_errors = [(f(x) − y)**2 for (x,y) in data]
    return sum(squared_errors)</pre>

  <p class="body"><a id="pgfId-1107612"></a>We can also visualize this cost function. Instead of looking at the vertical distances between points and the graph of the function, we can think of those distances as edges of squares. The area of each square is the squared error for that data point, and the total area of all squares is the result of <code class="fm-code-in-text">sum_squared_error</code>. The total area of squares in figure 14.9 shows the sum of the squared error between <code class="fm-code-in-text">test_data</code> and <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i>. (Note that the squares don’t look like squares because the x- and y-axes have different scales!)</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F09_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116373"></a>Figure 14.9 Picturing the sum of the squared error between a function and a data set</p>

  <p class="body"><a id="pgfId-1107618"></a>A <i class="fm-in-times-italic">y</i> value, which is twice as far from the graph in figure 14.9, contributes to the sum squared error by a factor of four. One of the reasons to prefer this cost function is that it penalizes poor fits more aggressively. For <i class="fm-in-times-italic">h</i>(<i class="fm-in-times-italic">x</i>) = 3<i class="fm-in-times-italic">x</i>, you can see that the squares are quite a bit bigger (figure 14.10).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F10_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116421"></a>Figure 14.10 Picturing the <code class="fm-code-in-text">sum_squared_error</code> for h(<i class="fm-in-times-italic">x</i>) = 3<i class="fm-in-times-italic">x</i> relative to the test data</p>

  <p class="body"><a id="pgfId-1107624"></a> It’s not worth drawing the squared errors for <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) = 1 − <i class="fm-in-times-italic">x</i> because the squares are so big they fill almost the whole chart area and overlap each other significantly. You can see, though, that the difference in value of the <code class="fm-code-in-text">sum_squared_error</code> is even more drastic for <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) and <i class="fm-in-times-italic">g</i>(<i class="fm-in-times-italic">x</i>) than for the difference in <code class="fm-code-in-text">sum_error</code> :</p>
  <pre class="programlisting">&gt;&gt;&gt; sum_squared_error(f,test_data)
2.105175107540148
&gt;&gt;&gt; sum_squared_error(g,test_data)
97.1078879283203</pre>

  <p class="body"><a id="pgfId-1107626"></a>The graph of <i class="fm-in-times-italic">y</i> = <i class="fm-in-times-italic">x</i><sup class="fm-superscript">2</sup> in figure 14.8 is clearly smooth, and it turns out that if you move the line by varying the parameters <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> that define it, the cost function changes “smoothly” as well. For this reason, we’ll continue using the <code class="fm-code-in-text">sum_squared_error</code> cost function.</p>

  <h3 class="fm-head1" id="heading_id_6"><a id="pgfId-1107628"></a><a id="id_8b6aktj22p0m"></a>14.1.3 Calculating cost for car price functions</h3>

  <p class="body"><a id="pgfId-1107629"></a>I’ll start by making an educated guess about how Priuses depreciate as they gain mileage. There are several different models of Toyota Priuses, but I would guess the average retail price is about $25,000. To make the calculation simple, our first, naïve model assumes they stay on the road for 125,000 miles, after which they are worth exactly $0. That means that the cars depreciate at an average rate of $0.20 per mile. That implies the price <i class="fm-in-times-italic">p</i> of a Prius in terms of its mileage <i class="fm-in-times-italic">x</i> is given by subtracting 0.2<i class="fm-in-times-italic">x</i> dollars of depreciation from the starting price of $25,000, and this means <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) is a linear function because it has the familiar form, <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i>, with <i class="fm-in-times-italic">a</i> = <span class="fm-in-cambria">−</span>0.2 and <i class="fm-in-times-italic">b</i> = 25,000:</p>

  <p class="fm-equation"><a id="pgfId-1107630"></a> <i class="fm-in-times-italic2">p</i>(<i class="fm-in-times-italic2">x</i>) = <span class="fm-in-cambria">−</span>0.2<i class="fm-in-times-italic2">x</i> + 25,000</p>

  <p class="body"><a id="pgfId-1107631"></a>Let’s see how this function looks on the graph by plotting it next to the CarGraph data (figure 14.11). You can find the data and the Python code to plot it in the source code for this chapter.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F11_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116475"></a>Figure 14.11 The scatter plot of price and mileage for used Priuses with my hypothetical depreciation function</p>

  <p class="body"><a id="pgfId-1107637"></a>Clearly, many of the cars in the data set have made it past my guess of 125,000 miles. That can mean that my guess of the depreciation rate was too high. Let’s try a depreciation rate of $0.10 per mile, implying a pricing function:</p>

  <p class="fm-equation"><a id="pgfId-1107638"></a> <i class="fm-in-times-italic2">p</i>(<i class="fm-in-times-italic2">x</i>) = <span class="fm-in-cambria">−</span>0.10<i class="fm-in-times-italic2">x</i> + 25,000</p>

  <p class="body"><a id="pgfId-1107639"></a>This isn’t perfect either. We can see on the graph in figure 14.12 that this function overestimates the price of the majority of the cars.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F12_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116520"></a>Figure 14.12 Plotting a different function that assumes a depreciation of $0.10 per mile</p>

  <p class="body"><a id="pgfId-1107645"></a>We can also experiment with the starting value, which we’ve assumed is $25,000. Anecdotally, a car loses much of its value the moment it drives off the lot, so a price of $25,000 might be an overestimation for a used car with very few miles on it. If the car loses 10% of its value when it drives off the lot, a price of $22,500 at zero mileage might give us better results (figure 14.13).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F13_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116565"></a>Figure 14.13 Testing a starting value of $22,500 for used Toyota Priuses</p>

  <p class="body"><a id="pgfId-1107651"></a>We can spend a lot of time speculating about what the best linear function is to fit the data, but to see if our speculations are improving, we need to use a cost function. Using the <code class="fm-code-in-text">sum_squared_error</code> function, we can measure which of our educated guesses is the closest to the data. Here are three pricing functions translated to Python code:</p>
  <pre class="programlisting">def p1(x):
    return 25000 − 0.2 * x

def p2(x):
    return 25000 − 0.1 * x

def p3(x):
    return 22500 − 0.1 * x</pre>

  <p class="body"><a id="pgfId-1107653"></a>The <code class="fm-code-in-text">sum_squared_error</code> function takes a function as well as a list of pairs of numbers representing the data. In this case, we want pairs of mileages and prices:</p>
  <pre class="programlisting">prius_mileage_price = [(p.mileage, p.price) for p in priuses]</pre>

  <p class="body"><a id="pgfId-1107655"></a>Using the <code class="fm-code-in-text">sum_squared_error</code> function for each of the three pricing functions, we can compare their quality of fit to the data:</p>
  <pre class="programlisting">&gt;&gt;&gt; sum_squared_error(p1, prius_mileage_price)
88782506640.24002
&gt;&gt;&gt; sum_squared_error(p2, prius_mileage_price)
34723507681.56001
&gt;&gt;&gt; sum_squared_error(p3, prius_mileage_price)
22997230681.560013</pre>

  <p class="body"><a id="pgfId-1110334"></a>These are some big values, roughly 88.7 billion, 34.7 billion, and 22.9 billion, respectively. Once again, the values don’t matter, only their relative sizes. Because the last one is the lowest, we can conclude that <code class="fm-code-in-text">p3</code> is the best of the three pricing functions. Given how unscientific I was when making up these functions, it seems likely I could keep guessing and find a linear function making the cost even lower. Rather than guessing and checking, we’ll look at how to explore the space of possible linear functions systematically.</p>

  <h3 class="fm-head1" id="heading_id_7"><a id="pgfId-1110865"></a><a id="id_2m9qpm4n9y1o"></a>14.1.4 Exercises</h3>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1110852"></a><b class="fm-exercise-head">Exercise 14.1</b>: Create a set of data points lying on a line and demonstrate that the <code class="fm-code-in-text1">sum_error</code> and <code class="fm-code-in-text1">sum_squared_error</code> cost functions both return exactly zero for the appropriate linear function.</p>

        <p class="fm-sidebar"><a id="pgfId-1110853"></a><b class="fm-exercise-head">Solution</b>: Here is a linear function and some points that lie on its graph:</p>
        <pre class="programlisting">def line(<i class="fm-in-times-italic1">x</i>):
    return 3*x−2
points = [(x,line(<i class="fm-in-times-italic1">x</i>)) for <i class="fm-in-times-italic1">x</i> in range(0,10)]</pre>

        <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F13_Orland_UN01.png"/></p>

        <p class="fm-sidebar"><a id="pgfId-1110858"></a>Both <code class="fm-code-in-text1">sum_error(line,points)</code> and <code class="fm-code-in-text1">sum_squared_error(line,points)</code> return zero because there is no distance from any of the points to the line.</p>
      </td>
    </tr>
  </table>

  <p class="bodye">  </p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1110861"></a><b class="fm-exercise-head">Exercise 14.2</b>: Calculate the value of the cost for the two linear functions, <i class="fm-in-times-italic1">x</i> + 0.5 and 2<i class="fm-in-times-italic1">x</i> − 1. Which one produces a lower sum squared error relative to <code class="fm-code-in-text1">test_data</code>, and what does that say about the quality of the fits?</p>

        <p class="fm-sidebar"><a id="pgfId-1110862"></a><b class="fm-exercise-head">Solution</b>:</p>
        <pre class="programlisting">&gt;&gt;&gt; sum_squared_error(lambda x:2*x−1,test_data)
23.1942461283472
&gt;&gt;&gt; sum_squared_error(lambda x:x+0.5,test_data)
16.607900877665685</pre>

        <p class="fm-sidebar"><a id="pgfId-1110864"></a>The function <i class="fm-in-times-italic1">x</i> + 0.5 produces a lower value for <code class="fm-code-in-text1">sum_squared_error</code>, so it is a better fit to the <code class="fm-code-in-text1">test_data</code>.</p>
      </td>
    </tr>
  </table>

  <p class="bodye">  </p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1110868"></a><b class="fm-exercise-head">Exercise 14.3</b>: Find a linear function <code class="fm-code-in-text1">p4</code> that fits the data even better than <code class="fm-code-in-text1">p1</code>, <code class="fm-code-in-text1">p2</code>, or <code class="fm-code-in-text1">p3</code>. Demonstrate that it is a better fit by showing the cost function is lower than for <code class="fm-code-in-text1">p1</code>, <code class="fm-code-in-text1">p2</code>, or <code class="fm-code-in-text1">p3</code>.</p>

        <p class="fm-sidebar"><a id="pgfId-1110869"></a><b class="fm-exercise-head">Solution</b>: The best fit we found so far is <code class="fm-code-in-text1">p3</code>, represented by <i class="fm-in-times-italic1">p</i>(<i class="fm-in-times-italic1">x</i>) = 22,500 − 0.1 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">x</i>. To get an even better fit, you can try tweaking the constants in this formula until the cost is reduced. One observation that you might make is that <code class="fm-code-in-text1">p3</code> was a better fit because we reduced the <i class="fm-in-times-italic1">b</i> value from 25,000 to 22,500. If we reduce it slightly further, the fit gets even better. If we define a new function <code class="fm-code-in-text1">p4</code> with a <i class="fm-in-times-italic1">b</i> value of 20,000</p>
        <pre class="programlisting">def p4(<i class="fm-in-times-italic1">x</i>):
    return 20000 − 0.1 * x</pre>

        <p class="fm-sidebar"><a id="pgfId-1110871"></a>it turns out the <code class="fm-code-in-text1">sum_squared_error</code> is even lower:</p>
        <pre class="programlisting">&gt;&gt;&gt; sum_squared_error(p4, prius_mileage_price)
18958453681.560005</pre>

        <p class="fm-sidebar"><a id="pgfId-1110873"></a>This is lower than the values for any of the three previous functions, demonstrating that it is a better fit to the data.</p>
      </td>
    </tr>
  </table>

  <h2 class="fm-head" id="heading_id_8"><a id="pgfId-1110876"></a><a id="id_1ui2u54u58iw"></a>14.2 Exploring spaces of functions</h2>

  <p class="body"><a id="pgfId-1107686"></a>We ended the last section by guessing some pricing functions of the form <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i>, where <i class="fm-in-times-italic">x</i> represents the mileage on a used Toyota Prius and <i class="fm-in-times-italic">p</i> is a prediction of its price. By choosing different values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> and graphing the resulting function, <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>), we could tell which choices were better than others. The cost function gave us a way of measuring<a id="marker-1107687"></a> how close the functions came to the data, rather than eyeballing it. Our goal in this section is to systematize the process of trying different values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> to make the cost function as small as possible.</p>

  <p class="body"><a id="pgfId-1107688"></a>If you did the last exercise from section 14.1 and searched manually for a better fit, you might have noticed that part of the challenge is tuning <i class="fm-italics">both</i> <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> simultaneously. If you remember from chapter 6, the collection of all functions like <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> form a 2D vector space. When you guess and check, you’re blindly picking points in various directions in this 2D space and hoping the cost function decreases.</p>

  <p class="body"><a id="pgfId-1107689"></a>In this section, we’ll try to understand the 2D space of possible linear functions by graphing the <code class="fm-code-in-text">sum_squared_error</code> cost function with respect to the parameters <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, which define a linear function. Specifically, we can plot the cost as a function of the two parameters <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, which define a choice of <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) (figure 14.14).</p>

  <p class="body"><a id="pgfId-1110946"></a>The actual function we’ll plot takes two numbers, <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, and returns one number, which is the cost of the function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i>. We call this function <code class="fm-code-in-text">coefficient_cost(a,b)</code> because the numbers <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> are <i class="fm-italics">coefficients</i>. To plot such<a id="marker-1110947"></a> a function, we use a heatmap like we did in chapter 12.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F14_Orland.png"/><br class="calibre15"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107694"></a>Figure 14.14 A pair of numbers, (<i class="fm-in-times-italic">a, b</i>), define a linear function. Comparing it to the fixed actual data produces the cost as a single number.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F15_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116638"></a>Figure 14.15 Costs for various values of the slope a and the lines represented by each</p>

  <p class="body"><a id="pgfId-1107697"></a>As a warm up, we can try to fit a function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> to the <code class="fm-code-in-text">test_data</code> data set we used before. This is a simpler problem because <code class="fm-code-in-text">test_data</code> doesn’t have as many data points, and because we’ve only got one parameter to tune: <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> is a linear function with the value of <i class="fm-in-times-italic">b</i> fixed at zero. The graph of a function of this form is a line through the origin and the coefficient <i class="fm-in-times-italic">a</i> controls its slope. That means that there’s only one dimension to explore, and we can plot the value of the sum squared error versus the value of <i class="fm-in-times-italic">a</i>, which is an ordinary function graph.</p>

  <h3 class="fm-head1" id="heading_id_9"><a id="pgfId-1107699"></a><a id="id_tc2571pm4v01"></a>14.2.1 Picturing cost for lines through the origin</h3>

  <p class="body"><a id="pgfId-1107700"></a>Let’s use the same <code class="fm-code-in-text">test_data</code> data set as before and compute the <code class="fm-code-in-text">sum_squared _error</code> from functions of the form <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i>. We can then write a function <code class="fm-code-in-text">test_data _coefficient_cost</code>, taking the parameter <i class="fm-in-times-italic">a</i>(the slope) and returning the cost for <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i>. To do this, we first create the function <i class="fm-in-times-italic">f</i> from the value of the input <i class="fm-in-times-italic">a</i> and then we can pass it and the test data into the <code class="fm-code-in-text">sum_squared_error</code> cost function:</p>
  <pre class="programlisting">def test_data_coefficient_cost(a):
    def f(x):
        return a * x
    return sum_squared_error(f,test_data)</pre>

  <p class="body"><a id="pgfId-1107702"></a>Each value of this function corresponds to a choice of the slope, <i class="fm-in-times-italic">a</i>, and, therefore, tells us the cost of a line we could plot alongside <code class="fm-code-in-text">test_data</code>. Figure 14.15 shows a scatter plot of a few values of <i class="fm-in-times-italic">a</i> and their corresponding lines. I’ve drawn attention to the slope of <i class="fm-in-times-italic">a</i> = −1, which produces the highest cost and the line with the worst fit.</p>

  <p class="body"><a id="pgfId-1107708"></a>The <code class="fm-code-in-text">test_data_coefficient_cost</code> function turns out to be a smooth function that we can plot over a range of <i class="fm-in-times-italic">a</i> values. The graph in figure 14.16 shows us that the cost gets lower and lower until it hits a minimum around <i class="fm-in-times-italic">a</i> = 2 and then it starts increasing.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F16_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107713"></a>Figure 14.16 A graph of cost vs. the slope a, showing the quality of fit for different slope values</p>

  <p class="body"><a id="pgfId-1107714"></a>The graph in figure 14.16 tells us the line through the origin producing the lowest cost and, therefore, the <i class="fm-italics">best fit</i>, which has roughly a slope of 2 (we’ll find the exact value shortly). To find the best linear function fitting the used car data, let’s look at the cost over a space with one more dimension.</p>

  <h3 class="fm-head1" id="heading_id_10"><a id="pgfId-1107716"></a><a id="id_7vdd8j9a81to"></a>14.2.2 The space of all linear functions</h3>

  <p class="body"><a id="pgfId-1107717"></a>We’re looking for a function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> that comes closest to predicting the price of a Prius based on its mileage as measured by the <code class="fm-code-in-text">sum_squared_error</code> function. To evaluate different choices of the coefficients <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, we need to first write a function <code class="fm-code-in-text">coefficient_cost(a,b)</code> that gives the sum squared error for <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> relative to the car data. This looks like the <code class="fm-code-in-text">test_data_coefficient_cost</code> function, except there are two parameters and we use a different data set:</p>
  <pre class="programlisting">def coefficient_cost(a,b):
    def <i class="fm-in-times-italic1">p</i>(<i class="fm-in-times-italic1">x</i>):
        return a * <i class="fm-in-times-italic1">x</i> + b
    return sum_squared_error(p,prius_mileage_price)</pre>

  <p class="body"><a id="pgfId-1107719"></a>Now, there’s a 2D space of pairs of coefficients (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>), any of which gives us a different candidate function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) to compare with the price data. Figure 14.17 shows two points in the <i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i> plane and the corresponding lines on the graph.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F17_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107724"></a>Figure 14.17 Different pairs of numbers (<i class="fm-in-times-italic">a, b</i>) correspond to different price functions</p>

  <p class="body"><a id="pgfId-1107725"></a>For every pair (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) and corresponding function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i>, we can compute the <code class="fm-code-in-text">sum_squared _error</code> function; that’s what the <code class="fm-code-in-text">coefficient_cost</code> function does for us in one step. This gives us a cost value for every point in the <i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i> plane that we can plot as a heat map (figure 14.18).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F18_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1107730"></a>Figure 14.18 Cost for the linear function as a heatmap over values of a and b</p>

  <p class="body"><a id="pgfId-1107731"></a>On this heatmap, you can see that when (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) are at their extremes, the cost function is high. The heatmap is darkest in the middle, but it’s not visually clear whether there’s a minimum value for the cost or exactly where it occurs. Fortunately, we have a method for finding where in the (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) plane the cost function is minimized−gradient descent.</p>

  <h3 class="fm-head1" id="heading_id_11"><a id="pgfId-1107733"></a><a id="id_izxqc5wrj6a4"></a>14.2.3 Exercises</h3>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1111274"></a><b class="fm-exercise-head">Exercise 14.4</b>: Find the exact formula for a line through the origin that passes through one point (3, 4). Do this by finding the function <i class="fm-in-times-italic1">f</i>(<i class="fm-in-times-italic1">x</i>) = <i class="fm-in-times-italic1">ax</i>, which minimizes the sum squared error relative to this one-point data set.</p>

        <p class="fm-sidebar"><a id="pgfId-1111275"></a><b class="fm-exercise-head">Solution</b>: There is one coefficient we need to find, which is <i class="fm-in-times-italic1">a</i>. The sum of squared error is the squared difference between <i class="fm-in-times-italic1">f</i>(3) = <i class="fm-in-times-italic1">a</i> <span class="fm-in-cambria">·</span> 3 and 4. This is (3 <i class="fm-in-times-italic1">a</i> − 4)<sup class="fm-superscript1">2</sup>, which expands to 9 <i class="fm-in-times-italic1">a</i><sup class="fm-superscript1">2</sup> − 24 <i class="fm-in-times-italic1">a</i> + 16. We can think of this as a cost function in terms of <i class="fm-in-times-italic1">a</i>, that is, <i class="fm-in-times-italic1">c</i>(<i class="fm-italics">a)</i> = 9 <i class="fm-in-times-italic1">a</i><sup class="fm-superscript1">2</sup> − 24 <i class="fm-in-times-italic1">a</i> + 16.</p>

        <p class="fm-sidebar"><a id="pgfId-1115146"></a>The best value of <i class="fm-in-times-italic1">a</i> is the one that minimizes this cost. That value of <i class="fm-in-times-italic1">a</i> causes the derivative of the cost function to be zero. Using the derivative rules from chapter 10, we find <i class="fm-in-times-italic1">c</i>'(<i class="fm-in-times-italic1">a</i>) = 18 <i class="fm-in-times-italic1">a</i> − 24. This is solved when <i class="fm-in-times-italic1">a</i> = 4/3, meaning our line of best fit is</p>

        <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F18_Orland_EQ01.png"/></p>

        <p class="fm-sidebar"><a id="pgfId-1111282"></a>This clearly contains the origin and the point (4, 3).</p>
      </td>
    </tr>
  </table>

  <p class="bodye">  </p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1111334"></a><b class="fm-exercise-head">Exercise 14.5</b>: Suppose we use a linear function to model the price of a sports car with respect to its mileage with coefficients (<i class="fm-in-times-italic1">a</i>, <i class="fm-in-times-italic1">b</i>) = (−0.4, 80000). In English, what does that say about how the car depreciates over time?</p>

        <p class="fm-sidebar"><a id="pgfId-1111335"></a><b class="fm-exercise-head">Solution</b>: The value of <i class="fm-in-times-italic1">ax</i> + <i class="fm-in-times-italic1">b</i> when <i class="fm-in-times-italic1">x</i> = 0 is just <i class="fm-in-times-italic1">b</i> = 80,000. That means that at a mileage of 0, we can expect the car to sell for $80,000. The value <i class="fm-in-times-italic1">a</i> of <span class="fm-in-cambria">−</span>0.4 means that the function value <i class="fm-in-times-italic1">ax</i> + <i class="fm-in-times-italic1">b</i> decreases at a rate of 0.4 units for every one unit increase in <i class="fm-in-times-italic1">x</i>. That means that the car’s value decreases, on average, by 40 cents for every one mile it is driven.</p>
      </td>
    </tr>
  </table>

  <h2 class="fm-head" id="heading_id_12"><a id="pgfId-1107750"></a><a id="id_m5tf16kea5f6"></a>14.3 Finding the line of best fit using gradient descent</h2>

  <p class="body"><a id="pgfId-1107751"></a>In chapter 12, we used the gradient descent algorithm to minimize a smooth function of the form <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>). In simpler terms, that meant finding the values of <i class="fm-in-times-italic">x</i> and <i class="fm-in-times-italic">y</i> that made the value of <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x, y</i>) as small as possible. Because we have a <code class="fm-code-in-text">gradient_descent</code> function implemented, we can simply pass it a Python version of a function we want to minimize, and it automatically finds the inputs that minimize it.</p>

  <p class="body"><a id="pgfId-1107752"></a>Now, we want to find the values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> that make the cost of <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> as small as possible, in other words, minimizing the Python function <code class="fm-code-in-text">coefficient _cost(a,b)</code>. When we plug <code class="fm-code-in-text">coefficient_cost</code> into our <code class="fm-code-in-text">gradient_descent</code> function, we get back the pair (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) such that <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> is the line of best fit. We can use the values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> that we find to plot the line <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> and visually confirm that it’s a good fit to the data.</p>

  <h3 class="fm-head1" id="heading_id_13"><a id="pgfId-1107754"></a><a id="id_cgt0mh7imdla"></a>14.3.1 Rescaling the data</h3>

  <p class="body"><a id="pgfId-1107755"></a>There’s one last tricky detail we need to deal with before applying gradient descent. The numbers we’ve been working with have drastically different sizes: the depreciation rates are between 0 and −1, the prices are in the tens of thousands, and the cost function returned results in the hundreds of billions. If we don’t specify otherwise, our derivative approximation is taken using a <code class="fm-code-in-text">dx</code> value of 10<sup class="fm-superscript"><span class="fm-in-cambria1">−</span>6</sup>. Because these numbers differ so greatly in magnitude, we can run into numerical errors if we try to run the gradient descent as is.</p>

  <p class="fm-callout"><a id="pgfId-1107756"></a><span class="fm-callout-head">Note</span> I won’t go into the details of the numerical issues that come up; my goal is to show you how to apply the math concepts, not to write robust numerical code. Instead, I’ll just show you how to get around this issue by reshaping the data we’re using.</p>

  <p class="body"><a id="pgfId-1107757"></a>Based on our intuition about the data set, we can figure out some conservative bounds on the values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, producing the line of best fit. The value <i class="fm-in-times-italic">a</i> represents the depreciation, so the best value probably has a magnitude greater than 0.5 or 50 cents per mile. The <i class="fm-in-times-italic">b</i> value represents the price of a Prius with zero miles on it and should be safely below $50,000.</p>

  <p class="body"><a id="pgfId-1107758"></a>If we define new variables <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i> by <i class="fm-in-times-italic">a</i> = 0.5 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">b</i> = 50,000 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic">d</i>, then when <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i> have a magnitude less than one, <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> should have a magnitude less than 0.5 and 50,000, respectively. For values of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> smaller than these values, the cost function goes no higher than 10<sup class="fm-superscript">13</sup>. If we divide the cost function result by 10<sup class="fm-superscript">13</sup> and express it in terms of <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i>, we have a new version of the cost function whose inputs and outputs all have absolute value between zero and one:</p>
  <pre class="programlisting">def scaled_cost_function(c,d):
    return coefficient_cost(0.5*c,50000*d)/1e13</pre>

  <p class="body"><a id="pgfId-1107760"></a>If we find the values of <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i> that minimize this scaled cost function, we can find the <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i> values minimizing the original function, using the facts that <i class="fm-in-times-italic">a</i> = 0.5 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">b</i> = 50,000 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic">d</i>.</p>

  <p class="body"><a id="pgfId-1115930"></a>This is a somewhat back-of-the-envelope approach, and there are more scientific ways to scale data to make it more numerically mangeable, one of which we’ll see in chapter 15. If you want to learn more, the usual process is called <i class="fm-italics">feature scaling</i> in machine learning<a id="marker-1115931"></a> literature. For now, we’ve got what we need−a function we can plug into the gradient descent algorithm.</p>

  <h3 class="fm-head1" id="heading_id_14"><a id="pgfId-1107764"></a><a id="id_4yh0kfd4gyn7"></a>14.3.2 Finding and plotting the line of best fit</h3>

  <p class="body"><a id="pgfId-1107765"></a>The function we’re going to optimize is <code class="fm-code-in-text">scaled_cost_function</code>, and we can expect the minimum to occur at a point (<i class="fm-in-times-italic">c</i>, <i class="fm-in-times-italic">d</i>), where |<i class="fm-in-times-italic">c</i>| &lt; 1 and |<i class="fm-in-times-italic">d</i>| &lt; 1. Because the optimal <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i> are reasonably close to the origin, we can start the gradient descent at (0,0). The following code finds the minimum, although it may take a while to run, depending on what kind of machine you’re using:</p>
  <pre class="programlisting">c,d = gradient_descent(scaled_cost_function,0,0)</pre>

  <p class="body"><a id="pgfId-1107767"></a>When it runs, it finds the following values for <i class="fm-in-times-italic">c</i> and <i class="fm-in-times-italic">d</i> :</p>
  <pre class="programlisting">&gt;&gt;&gt; (c,d)
(−0.12111901781176426, 0.31495422888049895)</pre>

  <p class="body"><a id="pgfId-1107769"></a>To recover <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i>, we need to multiply by the respective factors:</p>
  <pre class="programlisting">&gt;&gt;&gt; xa = 0.5*c
&gt;&gt;&gt; b = 50000*d
&gt;&gt;&gt; (a,b)
(−0.06055950890588213, 15747.711444024948)</pre>

  <p class="body"><a id="pgfId-1107771"></a>And, at last, we have our coefficients that we’ve been looking for! Rounding, we can say that the price function</p>

  <p class="fm-equation"><a id="pgfId-1107772"></a> <i class="fm-in-times-italic2">p</i>(<i class="fm-in-times-italic2">x</i>) = <span class="fm-in-cambria">−</span>0.0606 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic2">x</i> + 15,700</p>

  <p class="body"><a id="pgfId-1107773"></a>is the linear function that (approximately) minimizes the sum squared error over the whole data set of cars. It implies that the price of a Toyota Prius with zero miles on it is, on average, $15,700, and that Priuses depreciate at an average rate of just over 6 cents per mile. Figure 14.19 shows what the line looks like on a graph.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F19_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116788"></a>Figure 14.19 The line of best fit for the car price data</p>

  <p class="body"><a id="pgfId-1107779"></a>This looks at least as good as the other linear functions <i class="fm-in-times-italic">p</i><sub class="fm-subscript">1</sub>(<i class="fm-in-times-italic">x</i>), <i class="fm-in-times-italic">p</i><sub class="fm-subscript">2</sub>(<i class="fm-in-times-italic">x</i>), and <i class="fm-in-times-italic">p</i><sub class="fm-subscript">3</sub>(<i class="fm-in-times-italic">x</i>) that we tried, if not better. We can be sure that it’s a better fit to our data as measured by the cost function:</p>
  <pre class="programlisting">&gt;&gt;&gt; coefficient_cost(a,b)
14536218169.403479</pre>

  <p class="body"><a id="pgfId-1107781"></a>Having automatically found a line of best fit that minimizes the cost function, we can say that our algorithm “learned” how to value Priuses based on their mileage, and we achieved the main goal of this chapter.</p>

  <p class="body"><a id="pgfId-1107782"></a>There are a number of ways to compute linear regression to get this line of best fit, including a number of optimized Python libraries. Regardless of the methodology, they should get you to the same linear function that minimizes the sum of the squared error. I picked this specific methodology using gradient descent, both because it is a great application of a number of concepts we covered in part 1 and part 2, and also because it is highly generalizable. In the last section, I’ll show you one more application of gradient descent for regression, and we’ll make use of gradient descent and regression in the next two chapters as well.</p>

  <h3 class="fm-head1" id="heading_id_15"><a id="pgfId-1107784"></a><a id="id_gnpa1ur2mri0"></a>14.3.3 Exercises</h3>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1111615"></a><b class="fm-exercise-head">Exercise 14.6</b>: Use gradient descent to find the linear function that best fits the test data. Your resulting function should be close to 2<i class="fm-in-times-italic1">x</i> + 0, but not exactly, because the data was randomly generated around that line.</p>

        <p class="fm-sidebar"><a id="pgfId-1111616"></a><b class="fm-exercise-head">Solution</b>: First, we need to write a function that computes the cost of <i class="fm-in-times-italic1">f</i>(<i class="fm-in-times-italic1">x</i>) = <i class="fm-in-times-italic1">ax</i> + <i class="fm-in-times-italic1">b</i> relative to the test data in terms of the coefficients <i class="fm-in-times-italic1">a</i> and <i class="fm-in-times-italic1">b</i> :</p>
        <pre class="programlisting">def test_data_linear_cost(a,b):
    def f(x):
        return a*x+b
    return sum_squared_error(f,test_data)</pre>

        <p class="fm-sidebar"><a id="pgfId-1111618"></a>The values of <i class="fm-in-times-italic1">a</i> and <i class="fm-in-times-italic1">b</i> that minimize this function give us the linear function of best fit. We expect <i class="fm-in-times-italic1">a</i> and <i class="fm-in-times-italic1">b</i> to be close to 2 and 0, respectively, so we can plot a heat map around those points to understand the function we’re minimizing:</p>
        <pre class="programlisting">scalar_field_heatmap(test_data_linear_cost,-0,4,−2,2)</pre>

        <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F19_Orland_UN02.png"/></p>

        <p class="fm-figure-caption"><a id="pgfId-1111623"></a>The cost of <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> relative to the test data as a function of <i class="fm-in-times-italic">a</i> and <i class="fm-in-times-italic">b</i></p>
      </td>
    </tr>
  </table>

  <p class="bodye">  </p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1111649"></a>It looks like there’s a minimum to this cost function in the vicinity of (<i class="fm-in-times-italic1">a</i>, <i class="fm-in-times-italic1">b</i>) = (2,0), as expected. Using gradient descent to minimize this function, we can find the exact values:</p>
        <pre class="programlisting">&gt;&gt;&gt; gradient_descent(test_data_linear_cost,1,1)
(2.103718204728344, 0.0021207385859157535)</pre>

        <p class="fm-sidebar"><a id="pgfId-1111651"></a>This means the line of best fit to the test data is approximately 2.10372 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">x</i> + 0.00212.</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1107799"></a></p>

  <h2 class="fm-head" id="heading_id_16"><a id="pgfId-1107801"></a><a id="id_onnigx74p2lf"></a>14.4 Fitting a nonlinear function</h2>

  <p class="body"><a id="pgfId-1107802"></a>In the work we’ve done so far, there was no step that <i class="fm-italics">required</i> the price function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) to be linear. A linear function was a good choice because it was simple, but we can apply the same method with any function of one variable defined by two constants. As an example, let’s find the exponential function of best fit having the form <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">qe<sup class="fm-superscript2">rx</sup></i> and minimizing the sum of the squared error relative to the car data. In this equation, <i class="fm-in-times-italic">e</i> is the special constant 2.71828 . . ., and we’ll find the values of <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">r</i> that give us the best fit.</p>

  <h3 class="fm-head1" id="heading_id_17"><a id="pgfId-1107804"></a><a id="id_8do5v61f1ngx"></a>14.4.1 Understanding the behavior of exponential functions</h3>

  <p class="body"><a id="pgfId-1111885"></a>In case it’s been a while since you’ve worked with exponential functions, let’s do a quick review of how they work. You can recognize a function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) as exponential when the argument <i class="fm-in-times-italic">x</i> is in an exponent. For instance, <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i> is an exponential function, while <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">x</i><sup class="fm-superscript">2</sup> is not. In fact, <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) = 2<i class="fm-in-times-italic">x</i> is one of the most familiar exponential functions. The value of 2<i class="fm-in-times-italic">x</i> at each whole number of <i class="fm-in-times-italic">x</i> is 2 multiplied by itself <i class="fm-in-times-italic">x</i> times. Table 14.1 gives us some values of 2<i class="fm-in-times-italic">x</i>.</p>

  <p class="fm-table-caption"><a id="pgfId-1115423"></a>Table 14.1 Values of the familiar exponential function 2x</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre3">
      <col class="calibre4" span="1" width="10%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
    </colgroup>

    <tr class="calibre5">
      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115445"></a>x</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115447"></a>0</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115449"></a>1</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115451"></a>2</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115453"></a>3</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115455"></a>4</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115457"></a>5</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115459"></a>6</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115461"></a>7</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115463"></a>8</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115465"></a>9</p>
      </td>
    </tr>

    <tr class="calibre5">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115467"></a>2x</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115469"></a>1</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115471"></a>2</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115473"></a>4</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115475"></a>8</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115477"></a>16</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115479"></a>32</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115481"></a>64</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115483"></a>128</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115485"></a>256</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115487"></a>512</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1107853"></a>The number raised to the <i class="fm-in-times-italic">x</i> power is called the <i class="fm-italics">base</i>, so in the case of 2<i class="fm-in-times-italic">x</i>, the base is 2. If the base is greater than one, the function increases as <i class="fm-in-times-italic">x</i> increases, but if it is less than one, it decreases as <i class="fm-in-times-italic">x</i> increases. For instance, in (<span class="fm-in-cambria">½</span>)<i class="fm-in-times-italic">x</i>, each whole number value is half the previous as shown in table 14.2.</p>

  <p class="fm-table-caption"><a id="pgfId-1115729"></a>Table 14.2 Values of the decreasing exponential function (<span class="fm-in-cambria">½</span>)x</p>

  <table border="1" class="contenttable" width="100%">
    <colgroup class="calibre3">
      <col class="calibre4" span="1" width="10%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
      <col class="calibre4" span="1" width="9%"/>
    </colgroup>

    <tr class="calibre5">
      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115751"></a>x</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115753"></a>0</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115755"></a>1</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115757"></a>2</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115759"></a>3</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115761"></a>4</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115763"></a>5</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115765"></a>6</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115767"></a>7</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115769"></a>8</p>
      </td>

      <td class="fm-contenttable3" colspan="1" rowspan="1">
        <p class="fm-table-head"><a id="pgfId-1115771"></a>9</p>
      </td>
    </tr>

    <tr class="calibre5">
      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115774"></a>(<span class="fm-in-cambria">½</span>)<i class="fm-in-times-italic1">x</i></p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115776"></a>1</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115778"></a>0.5</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115780"></a>0.25</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115782"></a>0.125</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115784"></a>~0.06</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115786"></a>~0.03</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115788"></a>~0.015</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115790"></a>~0.008</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115792"></a>~0.004</p>
      </td>

      <td class="fm-contenttable2" colspan="1" rowspan="1">
        <p class="fm-table-body"><a id="pgfId-1115794"></a>~0.002</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1107903"></a>This is called an <i class="fm-italics">exponential decay</i>, and it’s more like<a id="marker-1107902"></a> what we want for our car depreciation model. An exponential decay means that the value of the function decreases by the same ratio over every <i class="fm-in-times-italic">x</i> interval of a fixed size. Once we have our model, it can tell us that a Prius loses half its value every 50,000 miles, implying that it’s worth is ¼ of its original price at 100,000 miles, and so on.</p>

  <p class="body"><a id="pgfId-1107904"></a>Intuitively, this could be a better way to model depreciation. Toyotas, which are reliable cars that last a long time, retain some value as long as they are driveable. Our linear model, by comparison, suggests that their value becomes negative after a long time (figure 14.20).</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F20a_Orland.png"/><img alt="" class="calibre14" src="../Images/CH14_F20b_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116854"></a>Figure 14.20 A linear model predicts negative values for Priuses, as compared with an exponential model that shows a positive value at any mileage.</p>

  <p class="body"><a id="pgfId-1107910"></a>The form of exponential function we can use is <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">qe<sup class="fm-superscript2">rx</sup></i> , where <i class="fm-in-times-italic">e</i> = 2.71828 . . . is the fixed base, and <i class="fm-in-times-italic">r</i> and <i class="fm-in-times-italic">q</i> are the coefficients we can adjust. (It might seem arbitrary or even inconvenient to use the base <i class="fm-in-times-italic">e</i>, but <i class="fm-italics">ex</i> is the standard exponential function, so it’s worth getting used to.) In the case of exponential decay, the value of <i class="fm-in-times-italic">r</i> is negative. Because <i class="fm-italics">er</i><sup class="fm-superscript"><span class="fm-in-cambria1">·</span>0</sup> = <i class="fm-in-times-italic">e</i><sup class="fm-superscript">0</sup> = 1, we have <i class="fm-in-times-italic">p</i>(<i class="fm-italics">0</i>) = <i class="fm-italics">qer</i><sup class="fm-superscript"><span class="fm-in-cambria1">·</span>0</sup> = <i class="fm-in-times-italic">q</i>, so <i class="fm-in-times-italic">q</i> still models the price of the Prius with zero miles. The constant <i class="fm-in-times-italic">r</i> decides the rate of depreciation.</p>

  <h3 class="fm-head1" id="heading_id_18"><a id="pgfId-1107912"></a><a id="id_73qkvkpmg6lh"></a>14.4.2 Finding the exponential function of best fit</h3>

  <p class="body"><a id="pgfId-1107913"></a>With the formula <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">qe<sup class="fm-superscript2">rx</sup></i> in mind, we can use our methodology from the previous sections to find the exponential function of best fit. The first step is to write a function that takes the coefficients <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">r</i> and returns the cost of the corresponding function:</p>
  <pre class="programlisting">def exp_coefficient_cost(q,r):
    def f(x):
        return q*exp(r*x)                           <span class="fm-combinumeral">❶</span>
    return sum_squared_error(f,prius_mileage_price)</pre>

  <p class="fm-code-annotation"><a id="pgfId-1115975"></a><span class="fm-combinumeral">❶</span> Python’s exp function computes the exponential function ex.</p>

  <p class="body"><a id="pgfId-1107916"></a>The next thing we need to do is choose a reasonable range for the coefficients <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">r</i> , which set the starting price and the depreciation rate, respectively. For <i class="fm-in-times-italic">q</i>, we expect it to be close to the value of <i class="fm-in-times-italic">b</i> from our linear model because both <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">b</i> represent the price of the car with zero miles on it. I’ll use a range from $0 to $30,000 to be safe.</p>

  <p class="body"><a id="pgfId-1107917"></a>The value of <i class="fm-in-times-italic">r</i> , which controls the depreciation rate, is a bit trickier to understand and set limits on. The equation <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) = <i class="fm-in-times-italic">qe<sup class="fm-superscript2">rx</sup></i> with a negative <i class="fm-in-times-italic">r</i> value implies that every time <i class="fm-in-times-italic">x</i> increases by −1/<i class="fm-in-times-italic">r</i> units, the price decreases by a <i class="fm-italics">factor</i> of <i class="fm-in-times-italic">e</i>, meaning it is multiplied by 1/ <i class="fm-in-times-italic">e</i> or about 0.36. (I’ve added an exercise at the end of the section to help you convince yourself of this!)</p>

  <p class="body"><a id="pgfId-1107918"></a>To be conservative, let’s say a car is reduced in price by a factor of 1/ <i class="fm-in-times-italic">e</i>, or to 36% of its original value, after 10,000 miles at the earliest. That would give us <i class="fm-in-times-italic">r</i> = 10<sup class="fm-superscript"><span class="fm-in-cambria1">−</span>4</sup>. A smaller <i class="fm-in-times-italic">r</i> value would mean a slower rate of depreciation. These benchmark magnitudes show us how to rescale the function, and if we divide by 10<sup class="fm-superscript">11</sup>, the cost values stay small as well. Here’s the implementation of the scaled cost function, and figure 14.21 shows a heat map of its output:</p>
  <pre class="programlisting">def scaled_exp_coefficient_cost(s,t):
    return exp_coefficient_cost(30000*s,1e<span class="fm-in-cambria">−</span>4*t) / 1e11

scalar_field_heatmap(scaled_exp_coefficient_cost,0,1,−1,0)</pre>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F21_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116929"></a>Figure 14.21 Cost as a function of the rescaled values of <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">r</i>, called <i class="fm-in-times-italic">s</i> and <i class="fm-in-times-italic">t</i>, respectively</p>

  <p class="body"><a id="pgfId-1107925"></a>The dark region at the top of the heatmap in figure 14.21 shows that the lowest cost occurs at a small value of <i class="fm-in-times-italic">t</i> and a value of <i class="fm-in-times-italic">s</i> somewhere in the middle of the range 0 to 1. We’re ready to plug the scaled cost function into the gradient descent algorithm. The outputs of the gradient descent function are the <i class="fm-in-times-italic">s</i> and <i class="fm-in-times-italic">t</i> values minimizing the cost function, and we can undo the scaling to get <i class="fm-in-times-italic">q</i> and <i class="fm-in-times-italic">r</i> :</p>
  <pre class="programlisting">&gt;&gt;&gt; s,t = gradient_descent(scaled_exp_coefficient_cost,0,0)
&gt;&gt;&gt; (s,t)
(0.6235404892859356, -0.07686877731125034)
&gt;&gt;&gt; q,r = 30000*s,1e<span class="fm-in-cambria">−</span>4*t
&gt;&gt;&gt; (q,r)
(18706.214678578068, <span class="fm-in-cambria">−</span>7.686877731125035e-06)</pre>

  <p class="body"><a id="pgfId-1107927"></a>This implies that the exponential function that best predicts the price of a Prius in terms of its mileage is approximately</p>

  <p class="fm-equation"><a id="pgfId-1107928"></a> <i class="fm-in-times-italic2">p</i>(<i class="fm-in-times-italic2">x</i>) = 18,700 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic2">e</i><sup class="fm-superscript"><span class="fm-in-cambria1">−</span>0.00000768</sup> <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic2">x</i></p>

  <p class="body"><a id="pgfId-1107929"></a>Figure 14.22 shows the graph with the actual price data.</p>

  <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F22_Orland.png"/></p>

  <p class="fm-figure-caption"><a id="pgfId-1116972"></a>Figure 14.22 The exponential function of best fit for a Prius and its mileage</p>

  <p class="body"><a id="pgfId-1107935"></a>You could argue that this model is even better than our linear model because it produces a lower sum squared error, meaning it fits the data (slightly) better according to the cost function:</p>
  <pre class="programlisting">&gt;&gt;&gt; exp_coefficient_cost(q,r)
14071654468.28084</pre>

  <p class="body"><a id="pgfId-1112784"></a>Using a nonlinear function like an exponential function is just one of the many variations on this regression technique. We could use other nonlinear functions, functions defined by more than two constants or data fit in more than 2 dimensions. In the next two chapters, we’ll continue to use cost functions to measure the quality of fit for regression models and then use gradient descent to make the fit as good as possible.</p>

  <h3 class="fm-head1" id="heading_id_19"><a id="pgfId-1112927"></a><a id="id_wt5r5k98mspz"></a>14.4.3 Exercises</h3>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1112915"></a><b class="fm-exercise-head">Exercise 14.7</b>: Confirm by choosing a sample value of <i class="fm-in-times-italic1">r</i> that <i class="fm-in-times-italic1">e</i><sup class="fm-superscript1"><span class="fm-in-cambria1">−</span><i class="fm-italics1">rx</i></sup> decreases by a factor of <i class="fm-in-times-italic1">e</i> every time <i class="fm-in-times-italic1">x</i> increases by 1/<i class="fm-in-times-italic1">r</i> units.</p>

        <p class="fm-sidebar"><a id="pgfId-1112916"></a><b class="fm-exercise-head">Solution</b>: Let’s take <i class="fm-in-times-italic1">r</i> = 3, so our test function is <i class="fm-in-times-italic1">e</i><sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>3<i class="fm-in-times-italic2">x</i></sup>. We want to confirm that this function decreases by a <i class="fm-italics">factor</i> of <i class="fm-in-times-italic1">e</i> every time <i class="fm-in-times-italic1">x</i> increases by <span class="fm-in-cambria">⅓</span> units. Defining the function in Python as follows</p>
        <pre class="programlisting">def test(x):
    return exp(−3*x)</pre>

        <p class="fm-sidebar"><a id="pgfId-1112918"></a>we can see that it starts at a value of 1 at <i class="fm-in-times-italic1">x</i> = 0 and decreases by a factor of <i class="fm-in-times-italic1">e</i> for every <span class="fm-in-cambria">⅓</span> we add to <i class="fm-in-times-italic1">x</i> :</p>
        <pre class="programlisting">&gt;&gt;&gt; test(0)
1.0
&gt;&gt;&gt; from math import e
&gt;&gt;&gt; test(1/3), test(0)/e
(0.36787944117144233, 0.36787944117144233)
&gt;&gt;&gt; test(2/3), test(1/3)/e
(0.1353352832366127, 0.1353352832366127)
&gt;&gt;&gt; test(1), test(2/3)/e
(0.049787068367863944, 0.04978706836786395)</pre>

        <p class="fm-sidebar"><a id="pgfId-1112920"></a>In each of these cases, adding <span class="fm-in-cambria">⅓</span> to the input of <code class="fm-code-in-text1">test</code> yields the same result as dividing the previous result by <i class="fm-in-times-italic1">e</i>.</p>
      </td>
    </tr>
  </table>

  <p class="bodye">  </p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1112923"></a><b class="fm-exercise-head">Exercise 14.8</b>: According to the exponential function of best fit, what percentage of the value of a Prius is lost every 10,000 miles?</p>

        <p class="fm-sidebar"><a id="pgfId-1112924"></a><b class="fm-exercise-head">Solution</b>: The price function is <i class="fm-in-times-italic1">p</i>(<i class="fm-in-times-italic1">x</i>) = 18,700 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">e</i><sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>0.00000768</sup> <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">x</i>, where the value <i class="fm-in-times-italic1">q</i> = $18,700, which represents the initial price and not how fast the price is decreasing. We can focus on the term <i class="fm-in-times-italic1">erx</i> = <i class="fm-in-times-italic1">e</i><sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>0.00000768</sup> <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">x</i> and see how much it changes over 10,000 miles. For <i class="fm-in-times-italic1">x</i> = 0, the value of this expression is 1, and for <i class="fm-in-times-italic1">x</i> = 10,000, the value is</p>
        <pre class="programlisting">&gt;&gt;&gt; exp(r * 10000)
0.9422186306357088</pre>

        <p class="fm-sidebar"><a id="pgfId-1112926"></a>This means that after 10,000 miles, the Prius is worth only 94.2% of its original price, a decrease of 5.8%. Given how the exponential function behaves, this will be the case over <i class="fm-italics">any</i> 10,000-mile increase in the mileage.</p>
      </td>
    </tr>
  </table>

  <p class="body"><a id="pgfId-1112944"></a></p>

  <table border="0" class="contenttable" width="100%">
    <tr class="calibre5">
      <td class="fm-contenttables" colspan="1" rowspan="1">
        <p class="fm-sidebar"><a id="pgfId-1112979"></a><b class="fm-exercise-head">Exercise 14.9</b>: Asserting that the retail price (the price at zero miles) is $25,000, what is the exponential function that best fits the data? In other words, fixing <i class="fm-in-times-italic1">q</i> = 25,000, what is the value of <i class="fm-in-times-italic1">r</i> yielding the best fit for <i class="fm-in-times-italic1">qe<sup class="fm-superscript">rx</sup></i> ?</p>

        <p class="fm-sidebar"><a id="pgfId-1112980"></a><b class="fm-exercise-head">Solution</b>: We can write a separate function that gives the cost of the exponential function in terms of the single unknown coefficient <i class="fm-in-times-italic1">r</i> :</p>
        <pre class="programlisting">def exponential_cost2(r):
    def f(x):
        return 25000 * exp(r*x)
    return sum_squared_error(f,prius_mileage_price)</pre>

        <p class="fm-sidebar"><a id="pgfId-1112982"></a>The following plot confirms that there’s a value of <i class="fm-in-times-italic1">r</i> between −10<sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>4</sup> and 0, which minimizes the cost function:</p>
        <pre class="programlisting">plot_function(exponential_cost2,−1e<span class="fm-in-cambria">−</span>4,0)</pre>

        <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F22_Orland_UN03.png"/></p>

        <p class="fm-sidebar"><a id="pgfId-1112943"></a>It looks like an approximate value of <i class="fm-in-times-italic1">r</i> = −10<sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>5</sup> minimizes the cost function. To automatically minimize this function, we need to write a one-dimensional version of the gradient descent or use another minimization algorithm. You can try that approach if you like, but because there’s only one parameter, we can simply guess and check to see that <i class="fm-in-times-italic1">r</i> = −1.12 <span class="fm-in-cambria">·</span> 10<sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>5</sup> is approximately the <i class="fm-in-times-italic1">r</i> value yielding the minimum cost. This implies the best fit function is <i class="fm-in-times-italic1">p</i>(<i class="fm-in-times-italic1">x</i>) = 25,000 <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">e</i><sup class="fm-superscript1"><span class="fm-in-cambria1">−</span>0.0000112</sup> <span class="fm-in-cambria">·</span> <i class="fm-in-times-italic1">x</i>. Here’s the graph of the new exponential fit, plotted with the raw price data:</p>

        <p class="fm-figure"><img alt="" class="calibre14" src="../Images/CH14_F22_Orland_UN04.png"/></p>
      </td>
    </tr>
  </table>

  <h2 class="fm-head" id="heading_id_20"><a id="pgfId-1112971"></a><a id="id_bz3vbd8uggxy"></a>Summary</h2>

  <ul class="calibre8">
    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107972"></a> <i class="fm-italics">Regression</i> is the process of finding a model to describe the relationship between various data sets. In this chapter, we use linear regression to approximate the price of a car from its mileage as a linear function.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107973"></a>For a set of many (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) data points, there is likely no line that passes through all of the points.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107974"></a>For a function <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) modeling the data, you can measure how close it comes to the data by taking the distance between <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>) and <i class="fm-in-times-italic">y</i> for the specified points (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>).</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107975"></a>A function measuring how well a model fits a data set is called a <i class="fm-italics">cost function</i>. A commonly used cost function is the sum of squared distances from (<i class="fm-in-times-italic">x</i>, <i class="fm-in-times-italic">y</i>) points to the corresponding model value <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>). The function that best fits the data has the lowest cost function.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107976"></a>Considering linear functions of the form <i class="fm-in-times-italic">f</i>(<i class="fm-in-times-italic">x</i>), every pair of coefficients (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) defines a unique linear function. There is a 2D space of such pairs and, therefore, a 2D space of lines to explore.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107977"></a>Writing a function that takes a pair of coefficients (<i class="fm-in-times-italic">a</i>, <i class="fm-in-times-italic">b</i>) and computes the cost of <i class="fm-in-times-italic">ax</i> + <i class="fm-in-times-italic">b</i> gives a function taking a 2D point and returning a number. Minimizing this function gives the coefficients defining the line of best fit.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1107978"></a>Whereas a linear function <i class="fm-in-times-italic">p</i>(<i class="fm-in-times-italic">x</i>) increases or decreases by a constant amount for constant changes in <i class="fm-in-times-italic">x</i>, an exponential function decreases or increases by a constant ratio for constant changes in <i class="fm-in-times-italic">x</i>.</p>
    </li>

    <li class="fm-list-bullet1">
      <p class="list"><a id="pgfId-1112948"></a>To fit an exponential equation to data, you can follow the same procedure as for a linear equation; you need to find the pair (<i class="fm-in-times-italic">q</i>, <i class="fm-in-times-italic">r</i>) yielding an exponential function <i class="fm-in-times-italic">qe<sup class="fm-superscript2">rx</sup></i> minimizing the cost function.</p>
    </li>
  </ul>
</body>
</html>
